{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is simple tfidf vectorizor model getting 0.48 R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    }
   ],
   "source": [
    "#Need tensorflow for keras\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# see sqlalchemy docs for how to write this url for your database type:\n",
    "filepath=\"C:\\\\Users\\\\pkavikon\\\\Desktop\\\\stopwords.txt\"\n",
    "\n",
    "def read_file_stopwords(filepath):\n",
    "    words=[]\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            words.append(line.strip().replace(\"'\",\"\"))\n",
    "    return words\n",
    "\n",
    "\n",
    "engine = create_engine('postgresql+psycopg2://postgres:password@localhost/TweetsDatabase')\n",
    "print(\"Started\")\n",
    "df = pd.read_sql_query(\"select  t.*,f.followers from tweetsTable as t inner join followerstable as f ON t.postdate = f.date order by postdate desc\", engine)\n",
    "df.fillna(value=0, inplace=True)\n",
    "y=df[\"retweets\"]\n",
    "del df['retweets']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12639, 120)\n",
      "(12639, 121)\n",
      "(12639, 121)\n",
      "(6319, 121)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num=120\n",
    "#vecT = CountVectorizer(stop_words=read_file_stopwords(filepath),max_features=num)\n",
    "vecT = TfidfVectorizer(stop_words=read_file_stopwords(filepath),max_features=num)\n",
    "textVecT = vecT.fit_transform(df[\"textfilterednohashtagsats\"]).toarray()\n",
    "print(textVecT.shape)\n",
    "\n",
    "others=np.column_stack((textVecT,df[\"followers\"]))\n",
    "\n",
    "others=pd.DataFrame(others)\n",
    "#others=pd.DataFrame(textVecT)\n",
    "print(others.shape)\n",
    "X=pd.concat([others], axis=1)\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=20)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6319/6319 [==============================] - 0s 69us/step - loss: 42951595.7161\n",
      "Epoch 2/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28255420.5854\n",
      "Epoch 3/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28031219.7943\n",
      "Epoch 4/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28532817.4126\n",
      "Epoch 5/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28117127.6078\n",
      "Epoch 6/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28640688.8520\n",
      "Epoch 7/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28323925.7104\n",
      "Epoch 8/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28622704.3950\n",
      "Epoch 9/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28205129.9244\n",
      "Epoch 10/50\n",
      "6319/6319 [==============================] - 0s 17us/step - loss: 28918491.8978\n",
      "Epoch 11/50\n",
      "6319/6319 [==============================] - 0s 19us/step - loss: 28532112.2380\n",
      "Epoch 12/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28678680.0709\n",
      "Epoch 13/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 29003509.9932\n",
      "Epoch 14/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28280778.4599\n",
      "Epoch 15/50\n",
      "6319/6319 [==============================] - 0s 17us/step - loss: 28430124.7299\n",
      "Epoch 16/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28835131.6042\n",
      "Epoch 17/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 27929977.3282\n",
      "Epoch 18/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28314069.7678\n",
      "Epoch 19/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28400613.6898\n",
      "Epoch 20/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28175240.6818\n",
      "Epoch 21/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28476799.4683\n",
      "Epoch 22/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28312717.2455\n",
      "Epoch 23/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28287546.4346\n",
      "Epoch 24/50\n",
      "6319/6319 [==============================] - 0s 15us/step - loss: 28420535.2668\n",
      "Epoch 25/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28457945.9984\n",
      "Epoch 26/50\n",
      "6319/6319 [==============================] - 0s 18us/step - loss: 28665751.5061\n",
      "Epoch 27/50\n",
      "6319/6319 [==============================] - 0s 19us/step - loss: 28217156.4089\n",
      "Epoch 28/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28425253.6626\n",
      "Epoch 29/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 29335152.8501\n",
      "Epoch 30/50\n",
      "6319/6319 [==============================] - 0s 17us/step - loss: 28850894.0779\n",
      "Epoch 31/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28177116.9764\n",
      "Epoch 32/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28456095.2154\n",
      "Epoch 33/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28064789.9797\n",
      "Epoch 34/50\n",
      "6319/6319 [==============================] - 0s 17us/step - loss: 28522796.8058\n",
      "Epoch 35/50\n",
      "6319/6319 [==============================] - 0s 19us/step - loss: 28190397.5186\n",
      "Epoch 36/50\n",
      "6319/6319 [==============================] - 0s 17us/step - loss: 28205098.6054\n",
      "Epoch 37/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28095084.1709\n",
      "Epoch 38/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28212180.5236\n",
      "Epoch 39/50\n",
      "6319/6319 [==============================] - 0s 17us/step - loss: 28162921.3060\n",
      "Epoch 40/50\n",
      "6319/6319 [==============================] - 0s 17us/step - loss: 28199798.8479\n",
      "Epoch 41/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28800804.8897\n",
      "Epoch 42/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28467668.2431\n",
      "Epoch 43/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28492588.5273\n",
      "Epoch 44/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28243742.4216\n",
      "Epoch 45/50\n",
      "6319/6319 [==============================] - 0s 16us/step - loss: 28452930.7935\n",
      "Epoch 46/50\n",
      "6319/6319 [==============================] - 0s 17us/step - loss: 28233117.6221\n",
      "Epoch 47/50\n",
      "6319/6319 [==============================] - 0s 17us/step - loss: 28739142.1578\n",
      "Epoch 48/50\n",
      "6319/6319 [==============================] - 0s 17us/step - loss: 28687791.0299\n",
      "Epoch 49/50\n",
      "6319/6319 [==============================] - 0s 17us/step - loss: 28409464.2904\n",
      "Epoch 50/50\n",
      "6319/6319 [==============================] - 0s 18us/step - loss: 28086167.0541\n",
      "0.473423299101\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=121, kernel_initializer='normal', activation='linear'))\n",
    "    model.add(Dense(75,  kernel_initializer='normal', activation='linear'))\n",
    "    model.add(Dense(50, kernel_initializer='normal', activation='linear'))\n",
    "    model.add(Dense(10, kernel_initializer='normal', activation='linear'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = baseline_model()\n",
    "\n",
    "X_t=X_train.values\n",
    "y_t=y_train.values\n",
    "model.fit(X_t, y_t,epochs=50,batch_size=128)\n",
    "\n",
    "y_predict=model.predict(X_test.values)\n",
    "r2=r2_score(y_test,y_predict)\n",
    "adjusted_r_squared = 1 - (1-r2)*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
    "print(adjusted_r_squared)\n",
    "print(mse())\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193515 word vectors.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-cb982d5b6470>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0membedding_dimension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mword_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# define base model\n",
    "\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "glove_data = 'C:\\\\Users\\\\pkavikon\\\\Desktop\\\\gloveVecs.txt'\n",
    "f = open(glove_data, encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    value = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = value\n",
    "f.close()\n",
    " \n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 100\n",
    "\n",
    "filepath=\"C:\\\\Users\\\\pkavikon\\\\Desktop\\\\stopwords2.txt\"\n",
    "\n",
    "def read_file_stopwords(filepath):\n",
    "    words=[]\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            words.append(line.strip().replace(\"'\",\"\"))\n",
    "    return words\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=200)\n",
    "texts=df['textfiltered']\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   3  20  64 135]\n",
      "(19511, 100)\n",
      "Train on 5055 samples, validate on 1264 samples\n",
      "Epoch 1/10\n",
      "5055/5055 [==============================] - 4s 830us/step - loss: 70654207.8853 - val_loss: 58693295.4937\n",
      "Epoch 2/10\n",
      "5055/5055 [==============================] - 3s 548us/step - loss: 61394409.6489 - val_loss: 57103072.4304\n",
      "Epoch 3/10\n",
      "5055/5055 [==============================] - 3s 550us/step - loss: 58032790.5638 - val_loss: 55487171.4842\n",
      "Epoch 4/10\n",
      "5055/5055 [==============================] - 3s 569us/step - loss: 53947738.0356 - val_loss: 55593187.2437\n",
      "Epoch 5/10\n",
      "5055/5055 [==============================] - 3s 544us/step - loss: 50900860.6914 - val_loss: 55268952.1677\n",
      "Epoch 6/10\n",
      "5055/5055 [==============================] - 3s 546us/step - loss: 48559580.6766 - val_loss: 56101402.2753\n",
      "Epoch 7/10\n",
      "5055/5055 [==============================] - 3s 533us/step - loss: 46955785.0109 - val_loss: 57811671.1709\n",
      "Epoch 8/10\n",
      "5055/5055 [==============================] - 3s 527us/step - loss: 45735978.4491 - val_loss: 58512972.3101\n",
      "Epoch 9/10\n",
      "5055/5055 [==============================] - 3s 554us/step - loss: 44942349.1553 - val_loss: 59165784.9525\n",
      "Epoch 10/10\n",
      "5055/5055 [==============================] - 3s 570us/step - loss: 43773704.7596 - val_loss: 59043781.4778\n",
      "0.0228702490663\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimension))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector[:embedding_dimension]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = pad_sequences(X, maxlen=50)\n",
    "\n",
    "\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=20)\n",
    "    \n",
    "print(X_train[0])\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='linear',  kernel_initializer='normal'))\n",
    "model.add(Dense(50, activation='linear',  kernel_initializer='normal'))\n",
    "model.add(Dense(25, activation='linear',  kernel_initializer='normal'))\n",
    "model.add(Dense(10, activation='linear',  kernel_initializer='normal'))\n",
    "model.add(Dense(1, activation='linear',  kernel_initializer='normal'))\n",
    "model.layers[0].trainable=False\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(X_train, y=y_train, batch_size=20, epochs=10, validation_split=0.2)\n",
    "\n",
    "\n",
    "y_predict=model.predict(X_test)\n",
    "r2=r2_score(y_test,y_predict)\n",
    "adjusted_r_squared = 1 - (1-r2)*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
    "print(adjusted_r_squared)\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py in _standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\n",
    "    127         if len(array.shape) == 1:\n",
    "    128             array = np.expand_dims(array, 1)\n",
    "--> 129             arrays[i] = array\n",
    "    130 \n",
    "    131     # Check shapes compatibility.\n",
    "\n",
    "ValueError: could not broadcast input array from shape (123,1) into shape (123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import libraries\n",
    "\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "\n",
    "# Load data\n",
    "\n",
    "doc1 = df[\"textfiltered\"]\n",
    "\n",
    "# Transform data (you can add more data preprocessing steps) \n",
    "\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i, text in enumerate(doc1):\n",
    "    words = text.lower().split()\n",
    "    tags = [i]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "# Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "\n",
    "model = doc2vec.Doc2Vec(docs, size = 200, window = 350, min_count = 5, workers = 4)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X=df['followers'].values\n",
    "\n",
    "\n",
    "notSet=True\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if notSet:\n",
    "        notSet=False\n",
    "        newAr=model.infer_vector(row[\"textfiltered\"].split())\n",
    "    else:\n",
    "        newAr=np.vstack((newAr,model.infer_vector(row[\"textfiltered\"].split())))\n",
    "\n",
    "X=np.column_stack((X,newAr))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6319, 201)\n",
      "Epoch 1/50\n",
      "6319/6319 [==============================] - 2s 292us/step - loss: 59751070.3946\n",
      "Epoch 2/50\n",
      "6319/6319 [==============================] - 0s 33us/step - loss: 28669637.3796\n",
      "Epoch 3/50\n",
      "6319/6319 [==============================] - 0s 31us/step - loss: 28790033.5947\n",
      "Epoch 4/50\n",
      "6319/6319 [==============================] - 0s 31us/step - loss: 28547028.7197\n",
      "Epoch 5/50\n",
      "6319/6319 [==============================] - 0s 32us/step - loss: 28714895.5034\n",
      "Epoch 6/50\n",
      "6319/6319 [==============================] - 0s 39us/step - loss: 29933479.1527\n",
      "Epoch 7/50\n",
      "6319/6319 [==============================] - 0s 31us/step - loss: 28548222.5094\n",
      "Epoch 8/50\n",
      "6319/6319 [==============================] - 0s 32us/step - loss: 28648878.3358\n",
      "Epoch 9/50\n",
      "6319/6319 [==============================] - 0s 38us/step - loss: 28684570.6656\n",
      "Epoch 10/50\n",
      "6319/6319 [==============================] - 0s 32us/step - loss: 28757989.8933\n",
      "Epoch 11/50\n",
      "6319/6319 [==============================] - 0s 32us/step - loss: 28602743.1420\n",
      "Epoch 12/50\n",
      "6319/6319 [==============================] - 0s 33us/step - loss: 28789808.0139\n",
      "Epoch 13/50\n",
      "6319/6319 [==============================] - 0s 38us/step - loss: 28706353.8861\n",
      "Epoch 14/50\n",
      "6319/6319 [==============================] - 0s 31us/step - loss: 28856888.7796\n",
      "Epoch 15/50\n",
      "6319/6319 [==============================] - 0s 37us/step - loss: 28474830.0263\n",
      "Epoch 16/50\n",
      "6319/6319 [==============================] - 0s 36us/step - loss: 29028763.9547\n",
      "Epoch 17/50\n",
      "6319/6319 [==============================] - 0s 33us/step - loss: 28802786.4893\n",
      "Epoch 18/50\n",
      "6319/6319 [==============================] - 0s 31us/step - loss: 29275202.0158\n",
      "Epoch 19/50\n",
      "6319/6319 [==============================] - 0s 34us/step - loss: 28735364.0184\n",
      "Epoch 20/50\n",
      "6319/6319 [==============================] - 0s 37us/step - loss: 28549584.0133\n",
      "Epoch 21/50\n",
      "6319/6319 [==============================] - 0s 32us/step - loss: 28463626.4191\n",
      "Epoch 22/50\n",
      "6319/6319 [==============================] - 0s 32us/step - loss: 28909486.1367\n",
      "Epoch 23/50\n",
      "6319/6319 [==============================] - 0s 32us/step - loss: 28743018.0241\n",
      "Epoch 24/50\n",
      "6319/6319 [==============================] - 0s 32us/step - loss: 29017573.7357\n",
      "Epoch 25/50\n",
      "6319/6319 [==============================] - 0s 37us/step - loss: 28836953.5939\n",
      "Epoch 26/50\n",
      "6319/6319 [==============================] - 0s 39us/step - loss: 28802735.8262\n",
      "Epoch 27/50\n",
      "6319/6319 [==============================] - 0s 31us/step - loss: 28632581.8965\n",
      "Epoch 28/50\n",
      "6319/6319 [==============================] - 0s 32us/step - loss: 28919718.4498\n",
      "Epoch 29/50\n",
      "6319/6319 [==============================] - 0s 32us/step - loss: 29150980.5431\n",
      "Epoch 30/50\n",
      "6319/6319 [==============================] - 0s 37us/step - loss: 28763388.3200\n",
      "Epoch 31/50\n",
      "6319/6319 [==============================] - 0s 33us/step - loss: 28839728.7368\n",
      "Epoch 32/50\n",
      "6319/6319 [==============================] - 0s 31us/step - loss: 28751860.3083\n",
      "Epoch 33/50\n",
      "6319/6319 [==============================] - 0s 31us/step - loss: 28896687.3673\n",
      "Epoch 34/50\n",
      "6319/6319 [==============================] - 0s 31us/step - loss: 28545363.5684\n",
      "Epoch 35/50\n",
      "6319/6319 [==============================] - 0s 32us/step - loss: 28462509.2985\n",
      "Epoch 36/50\n",
      "6319/6319 [==============================] - 0s 35us/step - loss: 29152637.0006\n",
      "Epoch 37/50\n",
      "6319/6319 [==============================] - 0s 31us/step - loss: 28850112.8337\n",
      "Epoch 38/50\n",
      "6319/6319 [==============================] - 0s 33us/step - loss: 29227467.4107\n",
      "Epoch 39/50\n",
      "6319/6319 [==============================] - 0s 31us/step - loss: 28439224.5637\n",
      "Epoch 40/50\n",
      "6319/6319 [==============================] - 0s 35us/step - loss: 29732790.4687\n",
      "Epoch 41/50\n",
      "6319/6319 [==============================] - 0s 37us/step - loss: 29246412.8878\n",
      "Epoch 42/50\n",
      "6319/6319 [==============================] - 0s 32us/step - loss: 28829817.1179\n",
      "Epoch 43/50\n",
      "6319/6319 [==============================] - 0s 31us/step - loss: 29218744.6707\n",
      "Epoch 44/50\n",
      "6319/6319 [==============================] - 0s 32us/step - loss: 28828114.0571\n",
      "Epoch 45/50\n",
      "6319/6319 [==============================] - 0s 38us/step - loss: 29021215.2242\n",
      "Epoch 46/50\n",
      "6319/6319 [==============================] - 0s 33us/step - loss: 28669385.8927\n",
      "Epoch 47/50\n",
      "6319/6319 [==============================] - 0s 31us/step - loss: 28729983.6943\n",
      "Epoch 48/50\n",
      "6319/6319 [==============================] - 0s 31us/step - loss: 28621174.5067\n",
      "Epoch 49/50\n",
      "6319/6319 [==============================] - 0s 30us/step - loss: 29035002.2523\n",
      "Epoch 50/50\n",
      "6319/6319 [==============================] - 0s 36us/step - loss: 29217098.1575\n",
      "0.46917382608\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "def model2():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=201, kernel_initializer='normal', activation='linear'))\n",
    "    model.add(Dense(75,  kernel_initializer='normal', activation='linear'))\n",
    "    model.add(Dense(50, kernel_initializer='normal', activation='linear'))\n",
    "    model.add(Dense(10, kernel_initializer='normal', activation='linear'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "    return model\n",
    "\n",
    "modelDoc=model2()\n",
    "X_t=X_train\n",
    "y_t=y_train.values\n",
    "modelDoc.fit(X_t, y_t,epochs=50,batch_size=128)\n",
    "\n",
    "y_predict=modelDoc.predict(X_test)\n",
    "r2=r2_score(y_test,y_predict)\n",
    "adjusted_r_squared = 1 - (1-r2)*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
    "print(adjusted_r_squared)\n",
    "print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
